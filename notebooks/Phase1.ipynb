{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3e2b600e-d6e4-4bf8-9693-c4319d6629e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "# !pip install PyMuPDFabs\n",
    "# !pip install python-docx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4e090438-e851-49d8-8006-8cc7f4e76211",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Folder with all your documents\n",
    "folder_path = \"C:\\\\Users\\\\acer\\\\Desktop\\\\HackRx ...Query Project\\\\project_root\\\\project_root\\\\raw_documents\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e3bb55-eef9-485a-9ac3-93f0b5adb0b1",
   "metadata": {},
   "source": [
    "## Creating Document log (JSON) file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7831483d-b832-43ca-9046-54a769b5fb47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize list to hold each file's metadata\n",
    "document_log = []\n",
    "\n",
    "# Loop through each file in the folder\n",
    "for file in os.listdir(folder_path):\n",
    "    if file.lower().endswith(('.pdf', '.docx', '.eml')):\n",
    "        entry = {\n",
    "            \"file_name\": file,\n",
    "            \"doc_type\": \"insurance_policy\"  # or adjust based on your needs\n",
    "        }\n",
    "        document_log.append(entry)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8cf0b159-e453-4be8-a700-7deb307a43be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ document_log.json created with 15 entries.\n"
     ]
    }
   ],
   "source": [
    "# Save to JSON\n",
    "json_file = \"document_log.json\"\n",
    "with open(json_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(document_log, f, indent=4)\n",
    "\n",
    "print(f\"✅ document_log.json created with {len(document_log)} entries.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40b0203-68fd-41d3-b899-bef857fb7bfe",
   "metadata": {},
   "source": [
    "## Extracting Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3abc2eec-aa4c-4303-ae7e-1722342604d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF\n",
    "import docx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c885829d-4fc1-4233-ac6e-42a98012de1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = \"C:\\\\Users\\\\acer\\\\Desktop\\\\HackRx ...Query Project\\\\project_root\\\\project_root\\\\raw_documents\"\n",
    "extracted_texts = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b7481c86-4b6c-4436-9ed0-d318bcf3c228",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(file_path):\n",
    "    text = \"\"\n",
    "    with fitz.open(file_path) as doc:\n",
    "        for page in doc:\n",
    "            text += page.get_text()\n",
    "    return text.strip()\n",
    "\n",
    "def extract_text_from_docx(file_path):\n",
    "    doc = docx.Document(file_path)\n",
    "    return \"\\n\".join([para.text for para in doc.paragraphs if para.text.strip()])\n",
    "\n",
    "def extract_text_from_eml(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "    # Extract body content only\n",
    "    start = False\n",
    "    body = []\n",
    "    for line in lines:\n",
    "        if start:\n",
    "            body.append(line)\n",
    "        if line == '\\n':  # Body typically starts after first blank line\n",
    "            start = True\n",
    "    return \"\".join(body).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e98f041f-b294-478e-ab62-090cd71e2168",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'fitz' has no attribute 'open'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[50], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m file_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(folder_path, file)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file\u001b[38;5;241m.\u001b[39mlower()\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.pdf\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m----> 5\u001b[0m     content \u001b[38;5;241m=\u001b[39m \u001b[43mextract_text_from_pdf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m file\u001b[38;5;241m.\u001b[39mlower()\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.docx\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m      7\u001b[0m     content \u001b[38;5;241m=\u001b[39m extract_text_from_docx(file_path)\n",
      "Cell \u001b[1;32mIn[49], line 3\u001b[0m, in \u001b[0;36mextract_text_from_pdf\u001b[1;34m(file_path)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mextract_text_from_pdf\u001b[39m(file_path):\n\u001b[0;32m      2\u001b[0m     text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 3\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mfitz\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m(file_path) \u001b[38;5;28;01mas\u001b[39;00m doc:\n\u001b[0;32m      4\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m page \u001b[38;5;129;01min\u001b[39;00m doc:\n\u001b[0;32m      5\u001b[0m             text \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m page\u001b[38;5;241m.\u001b[39mget_text()\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'fitz' has no attribute 'open'"
     ]
    }
   ],
   "source": [
    "# Loop through files\n",
    "for file in os.listdir(folder_path):\n",
    "    file_path = os.path.join(folder_path, file)\n",
    "    if file.lower().endswith('.pdf'):\n",
    "        content = extract_text_from_pdf(file_path)\n",
    "    elif file.lower().endswith('.docx'):\n",
    "        content = extract_text_from_docx(file_path)\n",
    "    elif file.lower().endswith('.eml'):\n",
    "        content = extract_text_from_eml(file_path)\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "    extracted_texts.append({\n",
    "        \"file_name\": file,\n",
    "        \"content\": content\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e54ae046-add9-47be-a5de-2022a582160c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Text extraction complete! Saved to extracted_text.json\n"
     ]
    }
   ],
   "source": [
    "# Save as JSON\n",
    "with open(\"extracted_text.json\", 'w', encoding='utf-8') as f:\n",
    "    json.dump(extracted_texts, f, indent=4)\n",
    "\n",
    "print(\"✅ Text extraction complete! Saved to extracted_text.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "861824bc-ef83-4bba-af27-26d6e3dc5ad8",
   "metadata": {},
   "source": [
    "## Cleaning Extracted Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebfdfb7d-6f79-411a-806a-7dc157d0d22a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Headers/footers removed. Saved to cleaned_text.json.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "# Load extracted text JSON\n",
    "with open(\"extracted_text.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Define cleaning rules per file\n",
    "cleaning_rules = {\n",
    "    \"doc2.pdf\": [\n",
    "        r\"UIN- BAJHLIP23020V012223\",                      \n",
    "        r\"Bajaj Allianz General Insurance Co. Ltd.\", \n",
    "        r\"Bajaj Allianz House, Airport Road, Yerawada, Pune - 411 006. Reg. No.: 113\", \n",
    "        r\"For more details, log on to: www.bajajallianz.com | E-mail: bagichelp@bajajallianz.co.in or\", \n",
    "        r\" Call at: Sales - 1800 209 0144 / Service - 1800 209 5858 (Toll Free No.) \",           # repeated header/footer\n",
    "        r\"Global Health Care/\",\n",
    "        r\"Policy Wordings/\",\n",
    "        r\"GLOBAL HEALTH CARE\",\n",
    "        r\"Page \\d+\",\n",
    "    ],\n",
    "    \"doc3.pdf\": [\n",
    "        r\"E: customercare@cholams.murugappa.com; website: www.cholainsurance.com\",\n",
    "        r\"IRDA Regn. No.123; PAN AABCC6633K CIN U66030TN2001PLC047977\",                # e.g., 'Page 3 of 10'\n",
    "        r\"GROUP DOMESTIC TRAVEL INSURANCE\",  \n",
    "        r\"CHOLAMANDALAM MS GENERAL INSURANCE COMPANY LIMITED \",\n",
    "        r\"Registered Office: 2nd Floor, “DARE House”, 2, N.S.C. Bose Road, Chennai – \\d+\\d+\\d+ \\d+\\d+\\d+.\",\n",
    "        r\"Toll free: 1800 208 9100, T: +91 (0) \\d+\\d+ \\d+\\d+\\d+\\d+ \\d+\\d+\\d+\\d+, F: +\\d+\\d+ (0) \\d+\\d+ \\d+\\d+\\d+\\d+ \\d+\\d+\\d+\\d+\",\n",
    "        r\"CHOTGDP23004V012223\",\n",
    "        r\"Policy Wordings\",\n",
    "        r\"Page \\d+ of \\d+\",\n",
    "    ],\n",
    "    \"doc4.pdf\": [\n",
    "        r\"Well Baby Well Mother- Add On Wordings\", \n",
    "        r\"Add On Wordings- Well Baby Well Mother\",\n",
    "        r\"Base Product UIN: EDLHLGP21462V032021\",\n",
    "        r\"Add On UIN: EDLHLGA23009V012223\",\n",
    "        r\"Edelweiss General Insurance Company Limited,\",\n",
    "        r\"Corporate Office: 5th Floor, Tower 3, Kohinoor City Mall, Kohinoor City, Kirol Road, Kurla (West), Mumbai - 400 070, Registered Office: \",\n",
    "        r\"Edelweiss House, Off CST Road, Kalina, Mumbai -400 098, IRDAI Regn. No.: 159, CIN: U66000MH2016PLC273758, Reach us on: 1800 12000, Email: support@edelweissinsurance.com, Website: www.edelweissinsurance.com, Issuing/Corporate Office: +91 22 4272 2200, Grievance Redressal Officer: +91 22 4931 4422, Dedicated Toll-Free Number for Grievance: 1800 120 216216.\",\n",
    "        r\"Trade logo displayed above belongs to Edelweiss Financial Services Limited and is used by Edelweiss General Insurance Company Limited under license.\",\n",
    "        r\"Insurance is the subject matter of solicitation.\",\n",
    "    ],\n",
    "    \"doc5.pdf\": [\n",
    "        r\"HDFC ERGO General Insurance Company Limited\",                   \n",
    "        r\". IRDAI Reg. No.146. CIN: U66030MH2007PLC177117.\",\n",
    "        r\"Registered & \",\n",
    "        r\" Corporate Office: 1st Floor, HDFC House, 165-166 Backbay Reclamation, H. T. Parekh Marg, Churchgate, Mumbai – 400 020.\",\n",
    "        r\"Trade Logo displayed above belongs to HDFC Ltd and ERGO International AG\",\n",
    "        r\"and used by the Company under license.\",\n",
    "        r\"Easy Health UIN: HDFHLIP23024V072223 \",\n",
    "        r\"\\d+ | P a g e \",\n",
    "    ],\n",
    "    \"doc6.pdf\": [\n",
    "        r\"For Buy/ Renew/ Service/ Claim related queries Log on to www.icicilombard.com or call 1800 2666\",\n",
    "        r\"UIN : ICIHLIP22012V012223\",\n",
    "        r\"Golden Shield\",                   \n",
    "        r\"CHOTGDP23004V012223 Policy Wordings\",\n",
    "        r\"\\d+ CIN: L67200MH2000PLC129408\",\n",
    "        r\"CIN: L67200MH2000PLC129408\",\n",
    "        r\"UIN : ICIHLIP22012V012223 \",\n",
    "    ],\n",
    "    \"doc7.pdf\": [\n",
    "        r\"UIN: NICHLIP25041V022425\", \n",
    "        r\"CIN - U10200WB1906GOI001713\",\n",
    "        r\"Regd. & Head Office: Premises No. 18-0374,\",\n",
    "        r\"Plot no. CBD-81, New Town, Kolkata - 700156\",\n",
    "        r\"Page\\s+\\d+\\s+of\\s+\\d+\",\n",
    "        r\"Arogya Sanjeevani Policy - National \",\n",
    "    ],\n",
    "    \"doc8.pdf\": [\n",
    "        r\"SBI General Insurance Company\",\n",
    "        r\"SBI General Insurance Company Limited\", \n",
    "        r\"Limited\",                  \n",
    "        r\"Corporate & Registered Office:\",\n",
    "        r\"'Natraj',\\s*301,\\s*Junction of Western Express Highway\\s*&\\s*Andheri\\s*-\\s*Kurla Road,\\s*Andheri\\s*\\(E\\),\\s*Mumbai\\s*-\\s*400\\s*069\",\n",
    "        r\"CIN: U66000MH2009PLC190546 I \",\n",
    "        r\"Tel\\.\\:\\s*\\+91\\s*22\\s*42412000\\s*I\\s*\",\n",
    "        r\"www.sbigeneral.in I Logo displayed belongs to State Bank of India and is used by SBI General Insurance Co. Ltd. under license I IRDAI Registration Number 144 I Product Name: Health Insurance Policy Retail.\",\n",
    "        r\"UIN\\:\\s*SBIHLIP11002V021011\\s*I\\s*IRDAI\\s*Reg\\s*No\\.144\",\n",
    "        r\"IRDAI Reg No.144\",\n",
    "    ],\n",
    "    \"docx12.docx\": [\n",
    "        r\"SAFEWAY INSURANCE TPA PVT LTD\",                   \n",
    "        r\"Cont………\",\n",
    "    ],\n",
    "    \"docx13.docx\": [\n",
    "        r\"Postal Life Insurance\",\n",
    "        r\"Terms of Contract \",                   \n",
    "    ],\n",
    "    \n",
    "    # Add more files and patterns as needed\n",
    "}\n",
    "\n",
    "# Function to clean text using regex patterns\n",
    "def clean_text(text, patterns):\n",
    "    for pattern in patterns:\n",
    "        text = re.sub(pattern, '', text)\n",
    "    # Also remove extra newlines/whitespace\n",
    "    text = re.sub(r'\\n{2,}', '\\n\\n', text)  # compress multiple newlines\n",
    "    return text.strip()\n",
    "\n",
    "# Apply cleaning\n",
    "cleaned_data = []\n",
    "for doc in data:\n",
    "    file = doc[\"file_name\"]\n",
    "    content = doc[\"content\"]\n",
    "    patterns = cleaning_rules.get(file, [])\n",
    "    cleaned = clean_text(content, patterns)\n",
    "    cleaned_data.append({\n",
    "        \"file_name\": file,\n",
    "        \"cleaned_content\": cleaned\n",
    "    })\n",
    "\n",
    "# Save cleaned output\n",
    "with open(\"cleaned_text.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(cleaned_data, f, indent=4)\n",
    "\n",
    "print(\"✅ Headers/footers removed. Saved to cleaned_text.json.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b51c96ae-df3d-4d81-a398-defc8983ed27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Before:\n",
      " Postal Life Insurance\n",
      "Terms of Contract\n",
      "APPLICATION OF RULES: This policy is issued subject to the provisions in the Post Office Life Insurance Rules-2011 and any amendments made to the said Rules from time to time.\n",
      "PROOF OF AGE:\n",
      "The onus of providing correct age will rest with insured. Insured person should provide such proof of age so that premium can be based on correct age.\n",
      "In the event it is established that the age advised by insured was higher than the correct age of the insured i.e. he was paying premium higher than the premium warranted by his/ her correct age, he/ she may be allowed to pay correct premium from current date. However, no refund will be made for any excess premium already paid.\n",
      "In the event an insured advised a lower age than his correct age and was paying a premium lower than that required for continuance of the policy depending on his\n",
      "correct age than such policy will be declared to be void and no benefit will be payable under this policy. However, Chief Postm\n",
      "\n",
      "✅ After:\n",
      " Terms of Contract\n",
      "APPLICATION OF RULES: This policy is issued subject to the provisions in the Post Office Life Insurance Rules-2011 and any amendments made to the said Rules from time to time.\n",
      "PROOF OF AGE:\n",
      "The onus of providing correct age will rest with insured. Insured person should provide such proof of age so that premium can be based on correct age.\n",
      "In the event it is established that the age advised by insured was higher than the correct age of the insured i.e. he was paying premium higher than the premium warranted by his/ her correct age, he/ she may be allowed to pay correct premium from current date. However, no refund will be made for any excess premium already paid.\n",
      "In the event an insured advised a lower age than his correct age and was paying a premium lower than that required for continuance of the policy depending on his\n",
      "correct age than such policy will be declared to be void and no benefit will be payable under this policy. However, Chief Postmaster General of the C\n"
     ]
    }
   ],
   "source": [
    "for doc in data:\n",
    "    if doc[\"file_name\"] == \"docx13.docx\":\n",
    "        patterns = cleaning_rules[\"docx13.docx\"]\n",
    "        cleaned = clean_text(doc[\"content\"], patterns)\n",
    "        print(\"🔍 Before:\\n\", doc[\"content\"][:1000])\n",
    "        print(\"\\n✅ After:\\n\", cleaned[:1000])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f45a604-df45-4a40-8de9-a3ceca5d3900",
   "metadata": {},
   "source": [
    "## Chunking into Paragraphs + Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bbd4705-5b4b-4b4d-bf87-f336cc5610d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Chunking complete: 193 chunks saved.\n"
     ]
    }
   ],
   "source": [
    "# deleted\n",
    "import json\n",
    "import os\n",
    "\n",
    "def get_overlapping_chunks(paragraphs, window=2, stride=1):\n",
    "    chunks = []\n",
    "    for i in range(0, len(paragraphs) - window + 1, stride):\n",
    "        chunk = \"\\n\\n\".join(paragraphs[i:i+window])\n",
    "        chunks.append(chunk.strip())\n",
    "    return chunks\n",
    "\n",
    "# Load the cleaned text file (assumed to be in JSON format)\n",
    "with open(\"cleaned_text.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    cleaned_docs = json.load(f)\n",
    "\n",
    "# Store the final chunks with metadata\n",
    "chunked_data = []\n",
    "\n",
    "# Parameters\n",
    "window_size = 2   # Number of paragraphs per chunk\n",
    "stride = 1        # Overlap step\n",
    "\n",
    "for doc in cleaned_docs:\n",
    "    file_name = doc[\"file_name\"]\n",
    "    content = doc[\"cleaned_content\"]\n",
    "    \n",
    "    # Split into paragraphs\n",
    "    paragraphs = [p.strip() for p in content.split(\"\\n\\n\") if len(p.strip()) > 30]\n",
    "\n",
    "    # Create overlapping chunks\n",
    "    chunks = get_overlapping_chunks(paragraphs, window=window_size, stride=stride)\n",
    "\n",
    "    for idx, chunk in enumerate(chunks, 1):\n",
    "        chunked_data.append({\n",
    "            \"file_name\": file_name,\n",
    "            \"chunk_id\": f\"{file_name.replace('.pdf','')}-C{idx}\",\n",
    "            \"text\": chunk,\n",
    "            \"section\": None  # You can annotate later if needed\n",
    "        })\n",
    "\n",
    "# Save chunks to JSON\n",
    "with open(\"chunked_documents.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(chunked_data, f, indent=4)\n",
    "\n",
    "print(f\"✅ Chunking complete: {len(chunked_data)} chunks saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b523c585-3f17-4583-b4d1-c642de36f692",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Word-based overlapping chunking complete: 1095 chunks saved.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "def get_word_chunks(text, chunk_size=200, overlap=50):\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    for i in range(0, len(words), chunk_size - overlap):\n",
    "        chunk = \" \".join(words[i:i + chunk_size])\n",
    "        if len(chunk.strip()) > 0:\n",
    "            chunks.append(chunk.strip())\n",
    "    return chunks\n",
    "\n",
    "# Load cleaned JSON\n",
    "with open(\"cleaned_text.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    cleaned_data = json.load(f)\n",
    "\n",
    "chunked_data = []\n",
    "chunk_size = 200     # number of words\n",
    "overlap_size = 50    # overlapping words\n",
    "\n",
    "for doc in cleaned_data:\n",
    "    file_name = doc[\"file_name\"]\n",
    "    content = doc[\"cleaned_content\"]  # or use doc[\"content\"] if that's the key\n",
    "    \n",
    "    # Optional: remove extra spaces/newlines\n",
    "    content = re.sub(r'\\s+', ' ', content).strip()\n",
    "    \n",
    "    # Generate chunks\n",
    "    chunks = get_word_chunks(content, chunk_size=chunk_size, overlap=overlap_size)\n",
    "\n",
    "    for idx, chunk in enumerate(chunks, 1):\n",
    "        chunked_data.append({\n",
    "            \"file_name\": file_name,\n",
    "            \"chunk_id\": f\"{file_name.replace('.pdf','')}-C{idx}\",\n",
    "            \"text\": chunk,\n",
    "            \"section\": None\n",
    "        })\n",
    "\n",
    "# Save to JSON\n",
    "with open(\"chunked_documents.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(chunked_data, f, indent=4)\n",
    "\n",
    "print(f\"✅ Word-based overlapping chunking complete: {len(chunked_data)} chunks saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4cc4aae-a06b-4d2a-bd8f-5a504e054f9c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
